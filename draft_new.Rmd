---
title: 'Predictability of Crash Modelling From the Data with Improved Quality Using Level Sets Surveillance System'
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(dplyr)
library(bda)
library(knitr)
library(kableExtra)
library(data.table)
```

#1. Introduction
<build up scenario for problems, and summarize contents of section 2 and 3>

#2. Evaluation Methodology
<describe how we evaluate data, the method, the process under different scenarios> 

##2.1 Level sets

Level set methods are widely used in image segmentations and processing. One of its applciations is for pattern recognitions and shape reconstructions. Instead of continuous and multidimentional space, same implementation of level set method is used as a measure of pattern recognitions for discrete space crashes analysis in this study.

Let $X$ be highway segments, $S_X$ be the support of $X$, $f(x)$ be the density of crashes at segment $X = x$, such that $\sum_{S_X} f(x) = 1$ (discrete spatial points). $\alpha$ is the top proportion of segments expected to be monitored. Then, the level set is defined as:

<center> $\gamma = \{ x \in \Omega |  f(x) \ge c \}$ </center>

where $\gamma$ is the set of segments $x$'s that $f(x) \ge c$, $c$ is a threshold satisfying $\underset{c}{\operatorname{inf}} \frac{\| \gamma \|}{\| S_X \|} \le \alpha$ and $c$ can be determined as $\underset{x \in \gamma}{\operatorname{min}}f(x)$.

Some drawbacks of using level set methods in discrete space data are such as approximation of desired levels and solution to the ties with the same rates. Considering these two main issues in this study, level $\hat{\alpha}$ of sample is contraint on the proportion of highway segments that is closer to desired $\alpha$ but not greater than $\alpha$. Since same ranks represent that crashes are equally likely to happen to these tied segments, ties should be all inclusive or exclusive in our desired set as the level changes. These two issues are under control with two general approaches mentioned previously. Figure 2.1 showed below demonstrates a comparison of crashes given different time periods under a level set method with $\alpha =$ 0.05 using highway data from Route 71, 2015. X-axis is the location ($x$ = Logmile) on this route whereas y-axis represents proportion of crashes $f(x)$. Threshold, $c$, (grey line) is set to be the minimal crash rate in the desired set $\gamma$. The true $\alpha$ levels for weekdays and weekend data are 0.0481 and 0.0377 with thresholds 0.0032 and 0.0039 repectively. 88 highway segments out of 1729 (or with a proportion of 0.0481) on route 71 with a crash rate equal or greater than 0.0032 approximately meets the desired level $\alpha =$ 0.05 for weekdays; similarily, 69 out of total 1729 segments on route 71 with a threshold equal or over 0.0039 satisfies this scenario. 
This setting is in a aspect of concertrating resources, say 0.05 in this example, and focusing on few highway segments with higher chance of having crashes with efficiency. 

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("wk_wkd.R", 14:44)
```
</center>

https://link-springer-com.libdata.lib.ua.edu/book/10.1007%2F978-3-642-15352-5  
https://link-springer-com.libdata.lib.ua.edu/book/10.1007%2F978-3-319-01712-9  

##2.2 Surveillance plots
Surveillance systems are built in respect of monitoring and capturing certain aberrations in the current or future process of tendency. These systems can be applied for detecting potential outbreak of epidemiological disease, and in essense used for evaluation of certain factors and profiles to acheive infection control in healthcare.

In conjuction with level set method, surveillance plots provide visualization of screening spatiotemporal highway data from distinct perspectives of monitoring crashes along highway segments. Under limited resources, say manpower, 10% or less of highway segments with the highest crash rates may be considered high priority and regularly kept under surveillance (fixed allocation based surveillance). Also, highway segments with a target percentage of crashes can also be determined and adjusted to capture different proportion of segments under various conditions (threshold based surveillance). Considering time influence, distributions of crashes can be expected either similiar or dissimilar. To observe and build up scenarios as described above, surveillance plots are essential to extend further exploratory and predictive analysis.

http://www.sciencedirect.com/science/article/pii/S187603411730206X  
http://www.sciencedirect.com/science/article/pii/S2093791116300610

###2.2.1 Fixed allocation based surveillance
The following is an example of fixed allocation based surveillance plots in two distinct way of presenting. Same data used for Figure 2.1 is also implemented in this example. Instead of single $\alpha$ level is observed in Figure 2.1, a sequence of $\alpha$ levels (from 0 to 1 with a increment of 0.01) are now concerned and showed in Figure 2.2. For (a) and (b), y-axis represents a cumulative proportion of crashes, and x-axis individually represents cumulative ordered segments size by crashes and top proportion of segments expected to be monitored, which is $\alpha$. The following paragraphs will describe this figures in detail.

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", c(54:57, 63, 66:67, 72:76, 82:83, 93:121))
```
</center>

Table 2.1 is obtained with arrangement by order and shows the composition for Figure 2.2 (a). *diff* is the difference between each segments; *rank* is ranked by proportion of crashes in individual segments; *p* is proportion of crashes in individual segments; *Log.size* and *log.rate* are the sum of *diff* and *p* with the same ranked segments respectively (e.g. in *rank* = 7, 4 segments are tied, therefore with a *log.size* = *diff* $\times$ 4 = 0.0078 and *log.rate* = *p* $\times$ 4 = 0.0145, since the *diff* is all equal on route 71). It is plotted cumulative segmental differences versus cumulative proportion of crashes in ordered segments, and readible to see the surveillance under different desired lengths of highway.

Figure 2.2 (b), in analog of Figure 2.2 (a), will be showed simultaneously in the parentheses. The grey vertical line is the desired length of highway, top 6.26 *logmile* (or $\alpha =$ 0.18) being under surveillance. Under this scenario, 82.26% and 86.92% of total crashes will be primarily on top 6.26 *logmile* (or top 18% of total segments) for weekdays and weekend respectively.

```{r echo = FALSE, warning = FALSE, message = FALSE}
kable(head(z3, 10), caption = "Table 2.1: Partial output for Figure 2.2 (a)")
```

###2.2.2 Threshold based surveillance
Figure 2.3 shows an example of threshold based surveillance plot for weekdays and weekend. Y-axis is the proportion of segments above a given threshold whereas x-axis is desired threshold, which is the crash rate on a segment. Grey line is set to be a desired threshold 0.0019. 9.62% and 10.99% (or 176 and 201 segments) of total segments of their crash rates will be above a desired threshold of 0.0019 repectively. In comparison with fixed allocation based surveillance, it is also flexible to set up a desired threshold *c* instead of a desired proportion of segments $\alpha$ to be monitored.

Unlike the implementation of fixed allocation based surveillance, threshold based surveillance is directly built from raw data instead of ranking data first. Under a set of threshold based surveillance segments, this is a problem of giving priority to individual segment since no distinction shows beyond the entire set. Both systems have their own disadvantages and advantages; simultaneously using two systems provides an improved aspect of measuring the quality of data to set up a comparatively optimal cut-off point for surveillance.

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", 127:150)
```
</center>

##2.3 Kolmogorov-Smirnov test
In analog of parametric method, *Q-Q* plots or *P-P* plots for assessing normality use quantile and percentile statistics to compare the magnitude of deviations between estimated and hypothetical distributions. Kolmogorov-Smirnov test (KS test) is a common nonparametric method to evaluate the goodness of fit between estimated and hypothetical distribution. For two-sample KS test, two random samples are compared to observe magnitude of differences based on their distributions. The primary approach of finding estimated distribution of a random variable is empirical cumulative distribution function (*ecdf*) or empirical distribution function (*edf*). An *ecdf* $S_n(x)$ is defined as:

$$S_n(x) = \sum_{i=1}^{n}{\textbf{I}(x_i \le x)}/n$$

where $\textbf{I}(x_i \le x)$ is a indicator function that equals to 1 if $x_i \le x$, and 0 otherwise.

For one-sample KS test $H_0: S_n(x) = F_X(x)$, the $D_n$ statistic is defined as:

\begin{equation}
D_n = \underset{x}{\operatorname{sup}} |S_n(x) - F_X(x)|
label{eq:KS}
\end{equation}

REMOVE: See \eqref{eq:KS} 

For two-sample KS test $H_0: S_m(x) = S_n(x)$, the $D_{m,n}$ statistic is defined as:

<center> $D_{m,n} = \underset{x}{\operatorname{max}} |S_m(x) - S_n(x)|$ </center>

If $D_{m,n} \ge d_0$, where $d_0 = c_{\alpha} \sqrt{\frac{1}{m} + \frac{1}{n}}$, $c_{\alpha}$ values can be found in any nonparametric materials, then $H_0$ is rejected, which implies $S_m(x)$ and $S_n(x)$ are from different distributions. Note that as $n, m \rightarrow \infty$, $c_{\alpha}$ is approximately $\sqrt{-\frac{1}{2} \operatorname{ln} (\frac{\alpha}{2})}$. Approximation of limiting $c_{\alpha}$ will be used for measure of distributions.

In this study, two-sample KS test is primarily conducted as a measure of association between distributions in analog of Least Significant Difference (LSD) for multiple comparisons in parametric methods. This test is contructed based on the snenarios used from a surveillance perspective whereas the surveillance is constructed based on the level set method; therefore, three ways of evaluations of highway data under different scenarios are seemingly distinct but closely tied together to organize a broaden perspecive of highway data structures.

Jean D. Gibbons, Subhabrata Chakraborti - Nonparametric Statistical Inference 4e (2003)

###2.3.1 Demonstration of conducting KS test
Recall from section 2.2, surveillance using level set is comparing the proportion of crashes under various $\alpha$ values. Continuing using the same data as examples showed earlier, $S_m(x)$ and $S_n(x)$ are *cdf*'s for weekdays and weekend with same number of $\alpha$ values (*m* = *n* = 101) using level set method repectively. Aussume significance level is 0.05, then, $D_{m,n} =$ 0.085 ($d_0 \approx$ 0.1911) with *p-value* 0.96, and therefore, $H_0: S_m(x) = S_n(x)$ is rejected. This is suggested that two distributions are indistinguishable. It is possible that model based on weekdays might have similar predictability with model based on weekend. Instead of two, one model might be sufficient for prediction and surveillance. 

The purpose of conducting a KS test on a fixed allocation based surveillance is that patterns of the top desired percentage of total segments for compared distributions are intrigued instead of the entire original highway patterns. In ordered segments, unnecessary segments can be easily discarded (beyond a $\alpha$ level), and the desired set (within $\alpha$ level) can be used to construct merely one model to predict weekdays and weekend patterns with better performance and precision. Using this procedure, model complexity and performance can both be ideally achieved.

##2.4 Jaccard Index plot
The Jaccard Index, or Jaccard similarity coefficient was introduced by Paul Jaccard and use as a measure of similarity of data. The Jaccard Index can be also extended to a Jaccard distance for measure of similarity in multidimensional space. General intersection-uion relationship will be applied in this paper. Suppose two samples are drawn, it is defined as a fraction of intersection and union sets of two samples.

<center> $J(A, B) = \frac{\| A \cap B \|}{\| A \cup B \|}$, where $A$ and $B$ represent two samples, $\|  \cdot \|$ is the size of a sample. </center>  

Jaccard, Paul (1912), The distribution of the flora in the alpine zone

In accordance with level set method, Jaccard Index can be calculated under a sequence of $\alpha$ levels and the Jaccard Index plot is showed below in Figure 2.4. Grey lines are at $\alpha$ level 0.1 and 0.2, with percentages of matched segments 47.93% and 59.03% respectively for the Jaccard Index of weekdays and weekend sets. Note that percentage of matched rate stays constant starts from roughly $\alpha$ = 0.37, this is caused by no crash records within the rest of highway segments.  

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", c(208:218, 228:241))
```
</center>

#3. Predictability of Crash Data
<Focus on spatial prediction. Given a time period, how well can we say *where* crash occurs>

##3.1 Data sources
The dataset used is composed of two subsets: *Highway* data and *Crash* data, and focus on the state of Arkansas region. *Highway* data recorded the highway surface characteristics across the entire highway system in Arkansas. Categories and variables such as location  (logmile, longitude, latitude), traffic characteristics (average daily traffic), surface characteristics (type of road, sign of route, type of operation, median type and number of lanes) and others are included in this set. *Crash* data recorded individual crash cases at the spot. It includes time features (crash date, crash time), location (latitude, longitude, city, county, street), highway features (class of property, classification of trafficway, road system, intersection, junction relation), environmental features (light condition, road way surface condition) and crash conditions (interaction type, crash manner, number of fatals, number of injuries, number of vehicles) and others are included in this set.

The data used in this paper is obtained from The Center for Advanced Public Safety (CAPS) at The University of Alabama. CAPS also cooperates with centers in other universities to work on various projects in different areas such as traffic safety and engineering and analytics, and has access to these numerous valuable data sources such as highway and crash data in different time and areas. This benefits this paper to conduct thorough analyses.

Grigorios Fountas a, Panagiotis Ch. Anastasopoulos, "A random thresholds random parameters hierarchical ordered probit analysis of highway accident injury-severities"

##3.2 Concentration of crashes: empirical performance measures
In previous demonstrations, crashes behaviors along the highway given different time periods, weekdays and weekend, are primarily showed a process of measuring data. Results are also presented that weekdays and weekend may consider both predictive, i.e., a model can be built using one of both sets, and this model may also capture a similar pattern for the other set. Other characteristrics of causes of crashes, e.g. time variation and segment variation, will be compared through the same process as demonstrations earlier to uncover patterns of latent connections.  

###3.2.1 Time variation
Given different time periods, distributions of crashes are varied. Several comparisons of distributions from time to time will be illustrated and only insignificant groups are summarized under significance level of 0.05 in following tables.

The magnitude of differences in distributions is generally unobservable, but subtle diversities in months become detected from the KS statistics (Table 3.1). December is comparatively distinct from any other months, and then January, June, July and November. On the other hand, February through May and August through October may be individually similar to each other respectively. Note that these classifications or similarities may follow a pattern of an academic year. A scenario can be established for distinctions between semester and off-semester periods. Also, off-semester months are varied in each other whereas semester months are similar; this results that semester months may be more appropriate for prediction than off-semester months due to the diversities of distributions in off-semester period.

```{r}
load("~/moy.RData")
kable(month.lvl.result2, caption = "Table 3.1: KS tests using level set for months of year") %>%
  kable_styling("striped", full_width = FALSE)
  
```

Classifications based on semester and off-semestere choice using distributions of months are shown in Figure 3.1. For semester class curves appear to be more consistent and slightly diverse as the proportion of segments increases whereas curves for off-semester class are more diverse across a sequence of $\alpha$ values. It is possible that using any one of semester months to construct the baseline is likely to have similar predictability in the same class. For off-semester months, choice of months as the baseline results in comparatively lower predictability.

<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", 125:157)
```
</center>

<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", 164:194)
```
</center>

Days of week are also of interest to be compared in the following (Figure 3.3) along with KS results from Table 3.2. Monday through Wednesday are comparatively stable and almost as same as each other in patterns; Thursday through Sunday are distinct from each other. A possible classification may be before-midweek and after-midweek patterns. A before-midweek based structure appears to be more representive and stable than the other group under this circumstance. Likewise, each of before-midweek based structure is expected to have higher predictability than of after-midweek based structure.

<center>
```{r}
load("~/dow.RData")
kable(dow.lvl.result2 %>% filter(p.value>=0.05), 
      caption = "Table 3.2: KS tests using level set for days of week") %>%
  kable_styling("striped", full_width = FALSE) 
  
```
</center>
<-------------->


<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", 113:146)
```
</center>

<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", 155:186)
```
</center>

Two primary time separations, days of week and months of year, are ways of using one data to make predictions of the other data. One other possibility is to separate time of day into different time periods. The purpose of screening data beforehand is to ensure the quality of data used is predictive to capture the true pattern of crashes based on certain characteristics.

Considering time of day divided into 2-hour, 4-hour, 8-hour and 12-hour periods starting from 00:00 for each category, it is more likely to go extreme condition while more categories are used, i.e., slightly difference in a specific time period and a highway segments tends to be significant. The boundary and range for different division for time of day are arbituary, other reasonable choices of time period are also applicable. For 2-hour periods, from 12:00 to 20:00, are showed to be insignificant but at the boundary of 0.05; for 4-hour periods, from 04:00 to 20:00 tend to have similar patterns; for 8-hour periods, 08:00 to 24:00, are also insignificant but close to the significant boundary; 12-hour periods are similar to each other.

<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 101:108))
kable(tod.lvl.result %>% filter(p.value>=0.05), 
      caption = "Table 3.3: KS tests using level set for time of day in 12 shifts") %>%
  kable_styling("striped", full_width = FALSE)

source_lines("G:/caps/tod.R", c(84:87, 101:108))
kable(tod.lvl.result %>% filter(p.value>=0.05), 
      caption = "Table 3.4: KS tests using level set for time of day in 6 shifts") %>%
  kable_styling("striped", full_width = FALSE)

source_lines("G:/caps/tod.R", c(89:92, 101:108))
kable(tod.lvl.result %>% filter(p.value>=0.05), 
      caption = "Table 3.5: KS tests using level set for time of day in 3 shifts") %>%
  kable_styling("striped", full_width = FALSE)

source_lines("G:/caps/tod.R", c(94:97, 101:108))
kable(tod.lvl.result %>% filter(p.value>=0.05), 
      caption = "Table 3.6: KS tests using level set for time of day in 2 shifts") %>%
  kable_styling("striped", full_width = FALSE)
```
</center>
<-------------->


<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 101:104, 116:129)) #2-hour
source_lines("G:/caps/tod.R", c(84:87, 101:104, 119:129)) #4-hour
source_lines("G:/caps/tod.R", c(89:92, 101:104, 119:129)) #8-hour
source_lines("G:/caps/tod.R", c(94:97, 101:104, 119:132)) #12-hour
```

</center>

###3.2.2 Segment variation
In time variations and previous descriptions, the distance between each primitive segments is fixed to show a thorough picture of distributions. Binning can determine a variety of distances to provide a less complex and flexible method of measurement. In most cases, binning is arduous to determine a suitable bin size because of its classification of observations at the boundaries; however, binning is simple to implement and adjust to requirements. Couple choices of binning size will be showed to observe the influences of predictions.

Recall from the comparison between weekdays and weekend data, bin sizes of 1 and 0.5 for weekdays and weekend are compared under the ordered bins and level sets (Figure 3.6 and 3.7). In the demonstration of KS test previously (Section 2.3.1), the statistic is $D_{m,n} =$ 0.085, the maximal differences between distributions for two bin sizes is 0.0177 and 0.0153 repectively, which are insigificant and expecxted, because binning will result data being smoother.

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", 306:350)
source_lines("G:/caps/wk_wkd.R", 354:390)
```
</center>

Bin size of 1 and 0.5 for days of week are showed in Figure 3.8 and 3.9 along with KS results in table 3.7 and 3.8, respectively. Both results show insignificance in distributions whether in ordered bins or level sets fashion. With bin size of 1, Sunday appears to slightly stand out compared to the others although it shows insignificant. For the rest of the days, each of them can be used as reference and may still have the smiliar predictablity.

<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", c(194:217))
kable(dow.lvl.result3, caption = "Table 3.7: KS tests using level set with bin size 1 for days of week") %>% kable_styling("striped", full_width = FALSE) 

source_lines("G:/caps/dow.R", c(258:276))
kable(dow.lvl.result4, caption = "Table 3.8: KS tests using level set with bin size 0.5 for days of week") %>% kable_styling("striped", full_width = FALSE)
```
</center>
<-------------->


<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", c(194:207, 223:250))
source_lines("G:/caps/dow.R", c(258:266, 282:310))
```
</center>

Considering different bin size 1 and 0.5 for distributions of crashes for months of year from Table 3.9 and 3.10 along with Figure 3.10 and 3.11, differences between months become smaller. For bin size 1, only June and August still have apparent gap in distributions. For bin size 0.5, February has the similar pattern as April, July and November before binning; here it results in difference between February and July after binning. This may account for classifications for observations at the boundaries that sometimes cause inconsistent results. January, March, September and December are not showed on these table, which may suggest a close performance on predictions using any of them because of the similarities with other months.

<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", c(200:226))
kable(moy.lvl.result3 %>% filter(p.value<=0.05), caption = "Table 3.9: KS tests using level set with bin size 1 for months of year") %>% kable_styling("striped", full_width = FALSE) 

source_lines("G:/caps/moy.R", c(266:288))
kable(moy.lvl.result4 %>% filter(p.value<=0.05), caption = "Table 3.10: KS tests using level set with bin size 0.5 for months of year") %>% kable_styling("striped", full_width = FALSE) 
```
</center>
<-------------->


<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", c(200:214, 232:259))
source_lines("G:/caps/moy.R", c(266:276, 293:320))
```
</center>

Considering different bin size 1 and 0.5 for distributions of crashes for time of day from Table 3.11-17 along with Figure 3.11-14. For 12 shifts, 20:00-22:00 and 02:00-06:00 with bin size 1, 00:00-04:00 with bine size 0.5, are apparently different in distributions from rest of the groups individually; for 6 shifts, only 04:00-08:00 and 20:00-24:00 with bin size 1 is different than the rest whereas all are similar for bin size 0.5; for 3 shifts and 2 shifts are individually showed similar in its own separations.

<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 140:141, 146, 150, 184:188))
kable(tod.lvl.result2 %>% filter(p.value<=0.05), 
      caption = "Table 3.11: KS tests using level set for time of day bin size 1 in 12 shifts") %>%  kable_styling("striped", full_width = FALSE)
source_lines("G:/caps/tod.R", c(79:82, 140, 142, 146, 150, 184:188))
kable(tod.lvl.result2 %>% filter(p.value<=0.05), 
      caption = "Table 3.12: KS tests using level set for time of day bin size 0.5 in 12 shifts") %>%  kable_styling("striped", full_width = FALSE)
```

```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(84:87, 140:141, 146, 150, 184:188))
kable(tod.lvl.result2 %>% filter(p.value<=0.05), 
      caption = "Table 3.13: KS tests using level set for time of day bin size 1 in 6 shifts") %>%  kable_styling("striped", full_width = FALSE)
#for bin size 0.5, all insignificant
```

```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(89:92, 140:141, 146, 150, 184:188))
kable(tod.lvl.result2 %>% filter(p.value>=0.05), 
      caption = "Table 3.14: KS tests using level set for time of day bin size 1 in 3 shifts") %>%  kable_styling("striped", full_width = FALSE)
source_lines("G:/caps/tod.R", c(89:92, 140, 142, 146, 150, 184:188))
kable(tod.lvl.result2 %>% filter(p.value>=0.05), 
      caption = "Table 3.15: KS tests using level set for time of day bin size 0.5 in 3 shifts") %>%  kable_styling("striped", full_width = FALSE)
```

```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(94:97, 140:141, 146, 150, 184:188))
kable(tod.lvl.result2, 
      caption = "Table 3.16: KS tests using level set for time of day bin size 1 in 2 shifts") %>%  kable_styling("striped", full_width = FALSE)
source_lines("G:/caps/tod.R", c(94:97, 140, 142, 146, 150, 184:188))
kable(tod.lvl.result2, 
      caption = "Table 3.17: KS tests using level set for time of day bin size 0.5 in 2 shifts") %>%  kable_styling("striped", full_width = FALSE)
```
</center>
<-------------->


<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 140:141, 143:175)) #2-hour, bin size = 1
source_lines("G:/caps/tod.R", c(142, 155:175)) #2-hour, bin size = 0.5
fig.des <- expression(paste("Figure 3.11: ordered bins with bin size = 1 (top) and 0.5 (bottom) for time of day in 12 shifts", sep = ""))
mtext(fig.des, side = 1, adj = 0.5, outer = TRUE)

source_lines("G:/caps/tod.R", c(84:87, 140:141, 143:175)) #4-hour, bin size = 1
source_lines("G:/caps/tod.R", c(142, 155:175)) #4-hour, bin size = 0.5
fig.des <- expression(paste("Figure 3.12: ordered bins with bin size = 1 (top) and 0.5 (bottom) for time of day in 6 shifts", sep = ""))
mtext(fig.des, side = 1, adj = 0.5, outer = TRUE)

source_lines("G:/caps/tod.R", c(89:92, 140:141, 143:175)) #8-hour, bin size = 1
source_lines("G:/caps/tod.R", c(142, 155:175)) #8-hour, bin size = 0.5
fig.des <- expression(paste("Figure 3.13: ordered bins with bin size = 1 (top) and 0.5 (bottom) for time of day in 3 shifts", sep = ""))
mtext(fig.des, side = 1, adj = 0.5, outer = TRUE)

source_lines("G:/caps/tod.R", c(94:97, 140:141, 143:175)) #12-hour, bin size = 1
source_lines("G:/caps/tod.R", c(142, 155:175)) #12-hour, bin size = 0.5
fig.des <- expression(paste("Figure 3.14: ordered bins with bin size = 1 (top) and 0.5 (bottom) for time of day in 2 shifts", sep = ""))
mtext(fig.des, side = 1, adj = 0.5, outer = TRUE)
```
</center>

##3.3 Predictive performance
After a variety of scenarios are compared and tools of surveillance plots and level levets are implemented, similarities and predictability that are discovered during the evaluation process will be tested in this section. A set of data, e.g. weekdays, which is considered more predctive, will be established as the baseline of a pseudo model, then the other set, e.g. weekend, will be tested based on this model.

##3.3.1 Predictability on time variation
Figure 3.15 shows the performance using one of the datasets, weekdays and weekend, as model, and the other as testing data. Using weekdays as model (black) has constantly higher predictability than using weekend as model (blue).

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", c(6:8, 396:415))
```
</center>

For days of week (Figure 3.16 and Table 3.18), Monday through Wednesday are combined and used as a model based on the level sets surveillance and KS results; Thursday through Sunday are compared to the reference model. For each testing datasets, Sunday is the best under Mon-Wed model, 10 segments with highest crash rates can be captured; Friday is the worst but still can be all captured using Mon-Wed model while up to 22 segments with highest crash rates on Friday are compared.

<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", c(8:17, 321:348))

kable(dow.mod[1:15,], caption = "Table 3.18: match rate table for Mon-Wed model") %>%  kable_styling("striped", full_width = FALSE)
```
</center>

For months of year (Figure 3.17 and Table 3.19), semester months (February through May and August through October) are combined and used as a model based on the analyses; rest of the months are compared to the model. January is the best using this model, 7 segments with highest crash rates on January can be captured; June, July, November and December are relatively predictive. Top 10 segments on June and July, 9 and 8 segments on November and December respectively, can be all captured.

<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", c(8:17, 334:361))

kable(moy.mod[1:10,], caption = "Table 3.19: match rate table for Feb-May and Aug-Oct model") %>%  kable_styling("striped", full_width = FALSE)
```
</center>

Time of day can subset into 2-hour, 4-hour and 8-hour periods (Figure 3.18, Table 3.20-22). The base models are established based on level sets and KS results described in previous sections. In 2-hour periods, 12:00-18:00 are used for model. Except for 00:00-06:00 period with rarity of crashes that can be all captured as expected, up to top 11 crash rate segments can be all inclusive for the other 2-hour periods. In 4-hour periods, 04:00-20:00 periods are used as a model, and the other 2 4-hour periods will be compared to reference. Similar to 2-hour periods, top 14 segments from 20:00-24:00 will match model predictions. In 8-hour periods, 08:00-24:00 are used as a model and all 9 segments are matched using this model.

<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(245:246))
source_lines("G:/caps/tod.R", c(79:82, 201:214, 241, 248:254, 256)) #2-hour
source_lines("G:/caps/tod.R", c(84:87, 217:228, 242, 248:254, 257)) #4-hour
source_lines("G:/caps/tod.R", c(89:92, 231:236, 243, 248:254, 258)) #8-hour
source_lines("G:/caps/tod.R", c(260:261))
```
</center>
<----------->

<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 201:214)) #2-hour
kable(tod.mod[1:10,], caption = "Table 3.20: 2-hour match rate table for 12:00-18:00 model") %>%  kable_styling("striped", full_width = FALSE)

source_lines("G:/caps/tod.R", c(84:87, 217:228)) #4-hour
kable(tod.mod[1:10,], caption = "Table 3.21: 4-hour match rate table for 04:00-20:00 model") %>%  kable_styling("striped", full_width = FALSE)

source_lines("G:/caps/tod.R", c(89:92, 231:236)) #8-hour
kable(tod.mod[1:10,], caption = "Table 3.22: 8-hour match rate table for 08:00-24:00 model") %>%  kable_styling("striped", full_width = FALSE)
```
</center>

##3.3.2 Predictability on segment variation
Different bin sizes of Logmile, 1 and 0.5, are compared to the model used similar distributions given different time periods. For the KS results within all groups are smiliar to each other, a model used all data is constructed and all datasets separated by different periods (e.g. Sunday through Saturday) are used as testing sets. For the other manipulation is that similar groups are combined and referenced as model, and the other groups will be testing using reference model.

For weekdays and weekend (Figure 3.19), weekdays are first as model, and weekend is tested (black); then weekend is used as model and then testing weekdays (blue). Two bin sizes, 1 (left) and 0.5 (right) are implemented. It appears that both sets are fairly predictive except for weekend model with bin size 0.5 for the reasons of the classifications of observations at boundaries and rarity of crashes on weekend.

<center>
```{r}
load("~/wk_wkd.RData")
source_lines("G:/caps/wk_wkd.R", c(306:310, 425:429, 431:442))
source_lines("G:/caps/wk_wkd.R", c(354:356, 425:429, 434:445))
```
</center>

Days of week (Figure 3.20) with bin sizes 1 (left) and 0.5 (right) are manipulated. For size 1, Monday through Saturday are insignificant compared to Sunday in accordance with KS results in Section 3.2.2 and thus combined together and built as model. Sunday is then tested. For size 0.5, days of week are all comparatively insignificant and thus whole week is used as a model and each day is then tested. For Sunday, it appears to have similar predictability for both sizes. 

<center>
```{r}
load("~/dow.RData")
source_lines("G:/caps/dow.R", c(359:371, 399:400, 402, 405:412))
source_lines("G:/caps/dow.R", c(376:395, 403, 405:411, 413:415))
```
</center>

Months of year (Figure 3.21) with bin size 1 (left) and 0.5 (right) are compared. For size 1, only February and April are distinct from each other and therefore are used as testing data. For size 0.5, January, March, September and December are combined and served as a reference, the rest of months are compared to it.

<center>
```{r}
load("~/moy.RData")
source_lines("G:/caps/moy.R", c(371:392, 425:428, 431:438))
source_lines("G:/caps/moy.R", c(399:418, 429:437, 439:442))
```
</center>

Time of day with bin size 1 and 0.5 for 2-hour, 4-hour, 8-hour divisions are showed in Figure 3.22. For 2-hour division (top), 00:00-02:00, 06:00-18:00 and 22:00-24:00 with size 1 are built as model whereas every periods with size 0.5 except for 00:00-04:00 are used. For 4-hour division (middle), 00:00-04:00 and 08:00-20:00 with size 1 are combined whereas all with size 0.5 are included and retested. For 8-hour division, all periods with each size are combined then retested based on the combined model.

<center>
```{r}
load("~/tod.RData")
source_lines("G:/caps/tod.R", c(79:82, 275:276, 279, 282:298, 327, 330:341))
source_lines("G:/caps/tod.R", c(79:82, 277, 280:295, 300, 327, 333:341))

source_lines("G:/caps/tod.R", c(84:87, 275:276, 303, 282:295, 312, 327, 333:341))
source_lines("G:/caps/tod.R", c(84:87, 277, 304, 282:290, 307:310, 313:315, 328, 333:339, 342))

source_lines("G:/caps/tod.R", c(89:92, 275:276, 318, 282:290, 307:310, 322, 328,  333:339, 342))
source_lines("G:/caps/tod.R", c(89:92, 277, 319, 282:290, 307:310, 322, 328,  333:339, 342:346))
```
</center>
